% !TEX root = thesis.tex

\chapter{System architecture}
\label{cha:system_architecture}

\section{Datasets}
\label{sec:datasets}

\subsection{Wikimedia dumps}
The Wikimedia foundation creates a dump of the publicly available data of the database behind every Wikimedia wiki.
The Wikimedia datasets used for our research are the one released the 1st of September 2015.

\todo{How much in detail should I go?}

\begin{table}[]
\centering
\caption{Size of wikipedia dump files.}
\label{tbl:wikidumps_size}
\begin{tabular}{@{}lrrr@{}}
\multicolumn{1}{c}{\textbf{}} & \textbf{Compressed size} & \textbf{Uncompressed size} & \textbf{Compression ratio} \\ \midrule
Pages dump              &     105.9 GB &   13\,387.9 GB &  0.79 \% \\
Category links table    &       1.7 GB &        12.1 GB & 13.77 \% \\
Page table              &       1.3 GB &         4.2 GB & 30.84 \% \\
Redirect table          &      96.9 MB &       364.3 MB & 26.60 \%
\end{tabular}
\end{table}

\paragraph{Pages XML dump}
The most important dataset for our purposes is the dump of all the pages, which include every revision created so far.
As you can see in Table~\ref{tbl:wikidumps_size}, this dataset is the most large: its uncompressed size is 13\,387.9 GB\@.
It is divided in 187 XML files, each of them containing several pages.
An extract of a dump is shown in Listing~\ref{lst:page_xml_extract}.
Every file contains a ``preamble'' with various metadata.
We can find the XML Schema location, the name of the project inside the \mintinline{xml}{<siteinfo>} tag, the list of namespaces up-to-date.
This is followed by a sequence of \mintinline{xml}{<page>} elements, each of them describing a Wikimedia page: its title, the namespace and the id.
After that there is sequence of \mintinline{xml}{<revision>} tags.
A revision is identified by an identifier, and usually have also reference (parent id) to the previous reference.
\todo{show that page revision graph is cyclic}
If this is the first revision of a page, this field will be left empty.
There are some other metadata fields, like the timestamp, the contributor and the comment.
Finally, we can find the actual source of the page in plain text format.

Thanks to the structure of the file, it is easy to analyze the file iterating over the XML elements without loading it entirely in memory.

\begin{listing}[]
    \inputminted[breaklines=true]{xml}{assets/page_xml_extract.xml}
    \caption{Extract of a dump XML document.}
    \label{lst:page_xml_extract}
\end{listing}

There are other datasets from Wikipedia that come in form of SQL dump files generated from the MySQL database powering Wikipedia.
\paragraph{Redirect table}
The redirects dataset is just a SQL dump of the corresponding MySQL table that powers the Wikimedia redirect mechanism.
It contains all the information about page redirects, in particular the id of the page from which the redirect starts, the name of the target page and its namespace.

\paragraph{Page table}
The page table contains various metadata about pages, updated to the date of the dump.
The features needed for our purposes are the page id, the namespace and the page title.

\paragraph{Category links table}
This table contain the membership pages to their category.
In particular, every row has the id of the page, its category and the type of link (for example, the link ``page'' denotes that a page belongs to a category, while ``subcat'' is used for categories belonging to other categories).

\section{Wikimedia pagecounts}
The Wikimedia page view statistics dataset, aka \emph{pagecounts-raw}, can be downloaded directly from the Wikimedia downloads directory\footnote{\url{https://dumps.wikimedia.org/other/pagecounts-raw/}}.
It is generated by the Wikimedia Squid caching servers: every time a reader request a page a log entry is created and the page is served.
Notice that the cache server does not know if the requested page is an article, a special page, a redirect or even if it exists.
Furthermore, if a page request contain spaces, these are replaced with underscores, because the MediaWiki software uses this convention.

It consists of many files, each of them containing the aggregated page view for a specific hour.
Files use a CSV dialect with \emph{spaces} as separators and they are compressed using the gzip format.
The encoding of this file is not documented. Many of them seem to use UTF-8, while others ISO-8859-2.
We use the first decoder for all the files, and whenever a Unicode endpoint cannot be found, we replace it with the official \textbf{U+FFFD REPLACEMENT CHARACTER}.

Every file is named according the following format:
\begin{verbatim}
    pagecounts-{year}{month}{day}-{hour}{minute}{second}.gz
\end{verbatim}
The content consists of four columns: the Wikimedia project, the requested page, the number of requests for that page in the last hour, and the size of the response.
The Wikimedia project name has of two parts.
The first is the abbreviation of the language of the project (e.g. \textbf{en} for English Wikimedia projects, \textbf{it} for Italian, etc.) and the second is the abbreviation of the name of the project prefixed by a dot (e.g. \textbf{.b} for wikibooks, \textbf{.d} for wiktionary, etc.).
In case of Wikipedia projects, the second part is omitted, and only the language abbreviation is kept.
Notice that this field is not always in lowercase: this is caused by clients that request url with mixed case letters.
A normalization is therefore required.

The second column, the requested page, is usually escaped by client browser replacing special bytes with an \textbf{\%xx} string, where \textbf{xx} is the hex representation, as per RFC 1808~\cite{rfc1808}.
It is necessary to decode this representation, since different client software can decide to encode or not certain characters.
For instances, client can decide to encode the page named \textbf{New\_Year's\_Day} as \textbf{New\_Year's\_Day} of \textbf{New\_Year\%27s\_Eve}.
This must be taken in consideration, since they refer to the same entity.

The third and the fourth column contains respectively the number of times the page has been served in the hour, and the size of bytes transferred to the clients.
This last field is not relevant for our purposes, but it will be kept because it may be useful for some other research.

In Listing~\ref{lst:pagecounts_extract} is shown an sample from the dump of the first hour of 2014.
The first line is straight forward: the page \textbf{Chomsky} has been seen once, and the size of the response was 98\,938 bytes.
The second line exhibit an example of encoding: the actual name of the page was \textbf{Chomsky–Schützenberger\_theorem} and all the non-ascii character plus the dash have been replaced with their hexadecimal representation.

\begin{listing}[]
    \inputminted[breaklines=true]{xml}{assets/pagecounts_extract_first_hour.txt}
    \caption{Extract from the first hour of the 2014 pagecounts-raw dataset (\textbf{pagecounts-20140101-000000.gz})}
    \label{lst:pagecounts_extract}
\end{listing}

\section{Microsoft academic graph}
\label{sec:mag_dataset}
Microsoft release a dump of the dataset powering Microsoft Academic Search in form of CSV files.
A brief overview of the size of the dataset is shown in Table~\ref{tbl:mag_size}.
\begin{table}[]
\centering
\caption{Size of Microsoft academic graph dataset files.}
\label{tbl:mag_size}
\begin{tabular}{@{}lrrr@{}}
\multicolumn{1}{c}{\textbf{}} & \textbf{Compressed size} & \textbf{Uncompressed size} & \textbf{Compression ratio} \\ \midrule
Papers table                &      9.0 GB &    27.4 GB & 32.71 \% \\
Paper references table      &      7.4 GB &    18.1 GB & 41.02 \% \\
Paper keywords table        &      1.8 GB &     5.3 GB & 34.51 \%
\end{tabular}
\end{table}
There are many entities in this dataset, and in our analysis we make use of most of them.

The \emph{Papers} listing contains information like the name of the paper, the publication date, its \ac{DOI} (if available), the journal or the conference where it appeared on.
There is a total of 120\,887\,833 papers, but only 35\,039\,319 of them contains a \ac{DOI} record (28.99\%) and, most important, only 33\,345\,644 of them have exactly one \ac{DOI} (27.58\%).
The fact that two papers share the same \ac{DOI} should be sought in the method used by Microsoft to index them.
It is important that a good percentage of papers have this identifier because our analysis relies on the fact that we can match publications that appear in Wikipedia, and this is done by matching the \ac{DOI} of a paper.

There is another important listing in this dataset, the \emph{PaperReferences} dump.
This table contains just two columns: the referring and the referred paper ids.

Finally, there are other files that contains information about the \emph{Journals}, \emph{Conferences}, \emph{Authors}, \emph{Keywords}, etc.


\todo{Explain that the ``conferences'' dataset is extremely biased: it seems that there are only compsci conferences}


\section{Software}
\label{sec:software}
\subsection{Sorting pagecounts}
\label{sub:Sorting pagecounts}
In order to sort the

\todo{Spark, python libs, sqlite}


\section{Infrastructure}
\label{sec:infrastructure}
\todo{My pc, ``adige'', unitn cluster}
