% !TEX root = thesis.tex

\chapter{System architecture}
\label{cha:system_architecture}
This section analyzes in detail the structure of the datasets, their problems and their peculiarities.
Also, we give a description of the machines used to execute our software, together with a description of what the various components do.

\section{Infrastructure}
\label{sec:infrastructure}
The various software components have been written and tested on a MacBook Pro.
Its specification can be found in Table~\ref{tbl:tech_specs}.
Its storage is limited to 500 GB and because of the size of our datasets it was not possible to run the programs on all of them.

For this reason University of Trento allowed us to use one of their machines called ``adige''.
Its hardware is not very different from the MacBook (see Table~\ref{tbl:tech_specs}).
Since we are going to need at least 866 GB just to store the Wikimedia page view statistics, the network office also allocated a 4 TB network disk.
This machine is always on, so we can launch the programs and retrieve the results as soon as they finish, without worrying of system reboots.

Even though the disk space is not a problem, the processing power is still an issue when we want to sort the page view statistics, as described in Sect.~\ref{sub:Sorting pagecounts}.
To mitigate this problem, we exploit the UniTN Cisca cluster.
It consist of circa 125 workstations, each of them having 4 cores available.
The technical specifications of the single workstation are not impressive (see Table~\ref{tbl:tech_specs}), but the fact that we can use more than 500 cores in total makes the difference.
\todo{explain the difficulty with working with torque}

\begin{table}[]
\centering
\caption{Technical specification of the machines used.}
\label{tbl:tech_specs}
\begin{tabular}{@{}lrrr@{}}
\toprule
\multicolumn{1}{c}{\textbf{}} & \textbf{MacBook Pro}           & \textbf{adige}                  & \textbf{Cisca workstation} \\ \midrule
Processor                     & 2.3GHz quad-core Intel i7 & 3.60GHz quad-core Intel i7 &  3.2GHz quad-core Intel i5  \\
Memory                        & 16 GB                          & 32 GB                           &   4 GB                         \\
Storage                       & 500 GB (SSD)                   & 793 GB (HD)                     &  20 GB                          \\
Network card                  & 1 GBps                         & 1 GBps                          &  1 GBps                          \\ \bottomrule
\end{tabular}
\todo{Add specs for workstations}
\end{table}


\section{Datasets}
\label{sec:datasets}

\subsection{Wikimedia dumps}
\label{sec:Wikipedia dumps}
The Wikimedia foundation creates a dump of the publicly available data of Wikipedia and all WMF projects on a regular basis.
English Wikipedia is dumped once a month, while smaller projects are often dumped twice a month.
The Wikimedia datasets used for our research are the one released the 1st of September 2015\footnote{\url{https://dumps.wikimedia.org/enwiki/20150901/}}.

\begin{table}[]
\centering
\caption{Size of wikipedia dump files.}
\label{tbl:wikidumps_size}
\begin{tabular}{@{}lrrr@{}}
\multicolumn{1}{c}{\textbf{}} & \textbf{Compressed size} & \textbf{Uncompressed size} & \textbf{Compression ratio} \\ \midrule
Pages dump              &     105.9 GB &   13\,387.9 GB &  0.79 \% \\
Category links table    &       1.7 GB &        12.1 GB & 13.77 \% \\
Page table              &       1.3 GB &         4.2 GB & 30.84 \% \\
Redirect table          &      96.9 MB &       364.3 MB & 26.60 \%
\end{tabular}
\end{table}

\paragraph{Pages XML dump}
The most important dataset for our purposes is the dump of all the pages, which include every revision created so far.
As you can see in Table~\ref{tbl:wikidumps_size}, this dataset is the most large: its uncompressed size is 13\,387.9 GB\@.
It is divided in 187 XML files compressed with 7zip, each of them containing several pages.
An extract of a dump is shown in Listing~\ref{lst:page_xml_extract}.
Every file contains a ``preamble'' with various metadata.
We can find the XML Schema location, the name of the project inside the \mintinline{xml}{<siteinfo>} tag, the list of namespaces up-to-date.
This is followed by a sequence of \mintinline{xml}{<page>} elements, each of them describing a Wikimedia page: its title, the namespace and the id.
Notice that if the title contains a space, it is not replaced by an underscore.
After that there is sequence of \mintinline{xml}{<revision>} tags.
A revision is identified by an identifier, and usually have also reference (parent id) to the previous reference.
\todo{maybe show that page revision graph is cyclic}
In the case of the first revision of a page, this field is left empty.
There are some other metadata fields, like the timestamp, the contributor and the comment.
Finally, we can find the actual source of the page in plain text format.

Thanks to the structure of the file, it is relatively easy to analyze the file iterating over the XML elements without loading it entirely in memory.

\begin{listing}[]
    \inputminted[breaklines=true]{xml}{assets/page_xml_extract.xml}
    \caption{Extract of a dump XML document.}
    \label{lst:page_xml_extract}
\end{listing}

\paragraph{Database dumps}
There are other datasets from Wikipedia that come in form of SQL dump files generated from the MySQL database underneath.
There is a minor discrepancy between the convention used the XML dumps and these SQL dumps regarding the title of a page.
While in the former it is not normalized in any way, in the latter spaces are replaced with underscores.
It is important to keep it in mind when analyzing these datasets.

The tables used in this project are three: redirects, pages and category links.
The \emph{redirect} dump has all the information needed for the redirect mechanism offered by the MediaWiki software.
It contains all the information about page redirects, in particular the id of the page from which the redirect starts, the title of the target page and its namespace.
It also contains other information, but they are not worth of description for our purposes.

The \emph{page} table contains up-to-date information about all the pages.
The relevant features the page id, the namespace and the page title.
There are other columns that are used by the MediaWiki software, such as the restrictions or the timestamp of the last edit, but they are not relevant for our analysis.

% The \emph{categorylinks} dump contain the membership of pages to categories.
% Every row represent a link between a page and a category.
% Indeed we can find the id of the page, the name of the category and the type of link (for example, the link ``page'' denotes that a page belongs to a category, while ``subcat'' is used for categories belonging to other categories).
% Other fields denotes the timestamp of the link creation and indexing metadata.

\subsection{Wikimedia pagecounts}
The Wikimedia page view statistics dataset, also known as \emph{pagecounts-raw}, can be downloaded directly from the Wikimedia downloads directory\footnote{\url{https://dumps.wikimedia.org/other/pagecounts-raw/}}.
It is a fairly huge source of data: the dataset compressed size of just the year 2014 is 866 GB, distributed in 8756 files.
It is generated by the Wikimedia cache servers: every time a reader request a page a log entry is created and the page is served.
Notice that the cache server does not know if the requested page is an article, a special page, a redirect or even if it exists.
Furthermore, if a page request contain spaces, these are replaced with underscores, because the MediaWiki software uses this convention.

Every file of this dataset contains the aggregated page view for a specific hour.
Files use a CSV dialect with \emph{spaces} as separators and they are compressed using the gzip format.
The encoding of this file is not documented.
Many of them seem to use UTF-8, while others ISO-8859-2. % chktex 8
We use the first decoder for all the files, and whenever a Unicode endpoint cannot be found, we replace it with the official \texttt{U+FFFD} replacement character.

Every file is named according the following format:
\begin{verbatim}
    pagecounts-{year}{month}{day}-{hour}{minute}{second}.gz
\end{verbatim}
The content consists of four columns: the Wikimedia project, the requested page, the number of requests for that page in the last hour, and the size of the response.
The Wikimedia project name has of two parts.
The first is the abbreviation of the language of the project (e.g. \textbf{en} for English Wikimedia projects, \textbf{it} for Italian, etc.) and the second is the abbreviation of the name of the project prefixed by a dot (e.g. \textbf{.b} for wikibooks, \textbf{.d} for wiktionary, etc.).
In case of Wikipedia projects, the second part is omitted, and only the language abbreviation is kept.
Notice that this field is not always in lowercase: this is caused by clients that request url with mixed case letters.
A normalization is therefore required.

The second column, the requested page, is usually escaped by client browser replacing special bytes with an \textbf{\%xx} string, where \textbf{xx} is the hex representation, as per RFC 1808~\cite{rfc1808}.
It is necessary to decode this representation, since different client software can decide to encode or not certain characters.
For instances, client can decide to encode the page named \textbf{New\_Year's\_Day} as \textbf{New\_Year's\_Day} or \textbf{New\_Year\%27s\_Eve}.
This must be taken in consideration, since they refer to the same entity.

The third and the fourth column contains respectively the number of times the page has been served in the hour, and the size of bytes transferred to the clients.
This last field is not relevant for our purposes, but it will be kept because it may be useful for some other research.

In Listing~\ref{lst:pagecounts_extract} is shown an sample from the dump of the first hour of 2014.
The first line is straight forward: the page \textbf{Chomsky} has been seen once, and the size of the response was 98\,938 bytes.
The second line exhibit an example of escaping: the actual name of the page was \textbf{Chomsky–Schützenberger\_theorem} and all the non-ascii character plus the dash have been replaced with their hexadecimal representation.

\begin{listing}[h]
    \inputminted[breaklines=true]{xml}{assets/pagecounts_extract_first_hour.txt}
    \caption{Extract from the first hour of the 2014 pagecounts-raw dataset (\textbf{pagecounts-20140101-000000.gz})}
    \label{lst:pagecounts_extract}
\end{listing}

\subsection{Microsoft academic graph}
\label{sec:mag_dataset}
Microsoft release a dump of the dataset powering Microsoft Academic Search in form of CSV files, compressed with zip.
A brief overview of the size of the dataset is shown in Table~\ref{tbl:mag_size}.

\begin{table}[h]
\centering
\caption{Size of Microsoft academic graph dataset files.}
\label{tbl:mag_size}
\begin{tabular}{@{}lrrr@{}}
\multicolumn{1}{c}{\textbf{}} & \textbf{Compressed size} & \textbf{Uncompressed size} & \textbf{Compression ratio} \\ \midrule
Papers table                &      9.0 GB &    27.4 GB & 32.71 \% \\
Paper references table      &      7.4 GB &    18.1 GB & 41.02 \% \\
Paper keywords table        &      1.8 GB &     5.3 GB & 34.51 \%
\end{tabular}
\end{table}

There are many entities in this dataset, and in our analysis we make use of some of them.

The \emph{Papers} listing contains information about the extracted papers like their title, the publication date, their \ac{DOI} (if available), the journal or the conference where they appeared on.
There is a total of 120\,887\,833 papers, but only 35\,039\,319 of them contains a \ac{DOI} record (28.99\%) and, most important, only 33\,345\,644 of them have exactly one \ac{DOI} (27.58\%).
The fact that two papers share the same \ac{DOI} should be sought in the method used by Microsoft to extract them.
It is important that a good percentage of papers have this identifier because our analysis relies on the fact that we can match publication identifiers that appear in Wikipedia, and this is done by matching the \ac{DOI} of a paper.

There is another important table in this dataset, \emph{PaperReferences}, which contains the references between papers.
It consists of just two columns: the referring and the referred paper identifiers.

Finally, there are other files that contains information about the \emph{Journals}, \emph{Conferences}, \emph{Authors}, \emph{Keywords}, etc.


\todo{Explain that the ``conferences'' dataset is extremely biased: it seems that there are only compsci conferences}


\section{Software}
\label{sec:software}
\subsection{Sorting pagecounts}
\label{sub:Sorting pagecounts}
A great amount of time has been spent on the \emph{pagecounts-raw} dataset.
Currently it is partitioned by the timestamp of each hour.
This representation is useful if you need to get the views for a certain hour, but as soon as you want to get the all the hourly views for an article it is simply not feasible.
Indeed, you would need to sequentially scan all the 8756 files (just for the year 2014), decompressing them on the fly.
Also, you could not perform any search optimization based on bisection, since the content inside the files is not ordered nor normalized.
Therefore we decided to reorganize this dataset, normalizing the content and sorting it by the (project, page, timestamp) \emph{key}, without losing information.

The program written for this purposes is a Spark job, using the Python programming language.

The idea is to normalize the content of the input files, then repartition the entities (i.e.\ the tuples containing the statistics for a given page in a project in a specific hour) using the ordering key.
Finally, every output partition is re-ordered locally and the result is saved into a different CSV file.

Our program will indeed create an \ac{RDD} for every input file, which are implicitly decompressed upon read operations.
Each line of the file is normalized: the project field is converted to lowercase and the page title is decoded according to RFC 1808~\cite{rfc1808}.
This transformation may cause multiple lines that refer to the same entity.
\todo{explain why}
Therefore it is necessary to merge these lines into one, summing the count of the views and the bytes transferred.
A \emph{key function} is also defined: it takes a tuple of the form $(timestamp, project, page, counts, bytes)$ and returns the $(project, page)$ key.

To repartition the entities, we need to fix the number of output partition.
We fixed this value to the number $N$ of input files.

Now we want to create a \emph{range partitioner}, a function that takes a tuple and returns the index of the new partition it belongs to.
Notice that Python implicitly defines an ordering between tuples by comparing their fields one at a time.
To create this function, we take an ordered sample of the keys of the first \ac{RDD} and enumerate it.
This enumeration is in fact a mapping between a key (project, page) and a partition index.
When looking for the partition for a new tuple $t$, it is sufficient to return the index corresponding to the last key which is less or equal to $t$.
Since the enumeration is ordered, the search can be optimized using the bisect algorithm.

At this point the \emph{union} of the input \acp{RDD} is repartitioned using this function, resulting in a ``coarse-sorted'' dataset: every tuple belongs to the right partition according to the ordering, but partitions are not yet locally sorted.
Indeed, the last operation needed is to sort tuples of each partition locally.
Values of these partition (i.e.\ tuples containing the timestamp, count and bytes) are also sorted by timestamp.
This results in a new \ac{RDD} normalized and sorted by $(page, project, timestamp)$.
The last step is to save the result in a CSV file.
The dataset is actually split in multiple gzipped CSV files, one for each partition, so that no more reshuffles are needed.

Since this job is highly parallelizable, we exploit the UniTN Cisca cluster described in Sect.~\ref{sec:infrastructure}.
Spark ships with a cluster manager which makes this operation more or less painless.
It is sufficient to start the Spark executable on the workstations and wait for them to connect to the master server.
Once they are ready, it is sufficient to launch the job on the master, and it automatically splits the tasks between the workers.

\subsection{Analyzing Wikipedia dumps}
\label{sub:Analyzing Wikipedia dumps}
The main analysis made on Wikipedia XML Dumps consist of extracting the history of the appearances of scholarly citations in Wikipedia articles.
To accomplish this, it is necessary to scan the text of all the revisions of every page and look for what seems to be a valid identifier.
As explained in Sect.~\ref{sec:Publication identifiers}, our search is limited to the following identifiers: \ac{DOI}, \ac{ISBN}, \emph{arXiv} and \ac{PMID}.
The program created uses a set of regular expressions that tries to match the identifiers' specifications.
This set has been originally developed by Aaron Halfaker and is part of the python-mwcites library\footnote{\url{https://github.com/mediawiki-utilities/python-mwcites}}.

When analyzing the XML dump, it is necessary to pay attention to what is being analyzed.
First of all, we do not want to analyze pages which are not articles, i.e.\ pages which namespace is not the \emph{mainspace}.
Secondly, the text of a revision should be stripped of comments, which may contain information which is not visible by the viewers.

Since we want to build the history of the appearances, we have to decide an ordering mechanism.
Since revisions have an id and a parent id, at first we tried to use the topological ordering of the directed graph induced by the ``parent of'' relation, from now on the \emph{history graph}.
On some pages this worked fine, but soon we discovered that it may contain cycles, so we decided to order the revisions by their timestamp.

The output is a CSV file, each line describing the appearance of an identifier inside a page.
It has the following form:
\begin{minted}[breaklines,breakafter=\,]{text}
en,12,Anarchism,doi,10.1080/07393140701510160,2013-05-26T09:12:47Z,2014-06-24T11:52:37Z
en,12,Anarchism,doi,10.1080/07393140701510160,2014-06-24T11:56:44Z,
[...]
\end{minted}
Every line contains the project name, the id of the page, the type of identifier, the actual identifier, the timestamp of the moment it appears and the timestamp when it ends.

In a successive run, to every line will be added a column representing how many views the identifier had in that period of time, exploiting the pagecounts dataset created.
