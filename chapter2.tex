% !TEX root = thesis.tex

\chapter{System architecture}
\label{cha:system_architecture}

\section{Datasets}
\label{sec:datasets}

\subsection{Wikimedia dumps}
The Wikimedia foundation creates a dump of the publicly available data of Wikipedia and all WMF projects on a regular basis.
English Wikipedia is dumped once a month, while smaller projects are often dumped twice a month.
The Wikimedia datasets used for our research are the one released the 1st of September 2015\footnote{\url{https://dumps.wikimedia.org/enwiki/20150901/}}.

\begin{table}[]
\centering
\caption{Size of wikipedia dump files.}
\label{tbl:wikidumps_size}
\begin{tabular}{@{}lrrr@{}}
\multicolumn{1}{c}{\textbf{}} & \textbf{Compressed size} & \textbf{Uncompressed size} & \textbf{Compression ratio} \\ \midrule
Pages dump              &     105.9 GB &   13\,387.9 GB &  0.79 \% \\
Category links table    &       1.7 GB &        12.1 GB & 13.77 \% \\
Page table              &       1.3 GB &         4.2 GB & 30.84 \% \\
Redirect table          &      96.9 MB &       364.3 MB & 26.60 \%
\end{tabular}
\end{table}

\paragraph{Pages XML dump}
The most important dataset for our purposes is the dump of all the pages, which include every revision created so far.
As you can see in Table~\ref{tbl:wikidumps_size}, this dataset is the most large: its uncompressed size is 13\,387.9 GB\@.
It is divided in 187 XML files compressed with 7zip, each of them containing several pages.
An extract of a dump is shown in Listing~\ref{lst:page_xml_extract}.
Every file contains a ``preamble'' with various metadata.
We can find the XML Schema location, the name of the project inside the \mintinline{xml}{<siteinfo>} tag, the list of namespaces up-to-date.
This is followed by a sequence of \mintinline{xml}{<page>} elements, each of them describing a Wikimedia page: its title, the namespace and the id.
Notice that if the title contains a space, it is not replaced by an underscore.
After that there is sequence of \mintinline{xml}{<revision>} tags.
A revision is identified by an identifier, and usually have also reference (parent id) to the previous reference.
\todo{show that page revision graph is cyclic}
If this is the first revision of a page, this field will be left empty.
There are some other metadata fields, like the timestamp, the contributor and the comment.
Finally, we can find the actual source of the page in plain text format.

Thanks to the structure of the file, it is easy to analyze the file iterating over the XML elements without loading it entirely in memory.

\begin{listing}[]
    \inputminted[breaklines=true]{xml}{assets/page_xml_extract.xml}
    \caption{Extract of a dump XML document.}
    \label{lst:page_xml_extract}
\end{listing}

\paragraph{Database dumps}
There are other datasets from Wikipedia that come in form of SQL dump files generated from the MySQL database powering Wikipedia.
There is a minor discrepancy between the convention used the XML dumps and these SQL dumps regarding the title of a page.
While in the former it is not normalized in any way, in the latter spaces are replaced with underscores.
It is important to keep it in mind when analyzing these datasets.

The tables used in this project are three: redirects, pages and category links.
The \emph{redirect} dump has all the information needed for the redirect mechanism offered by the MediaWiki software.
It contains all the information about page redirects, in particular the id of the page from which the redirect starts, the title of the target page and its namespace.
It also contains other information, but they are not worth of description for our purposes.

The \emph{page} table contains up-to-date information about all the pages.
The relevant features the page id, the namespace and the page title.
There are other columns that are used by the MediaWiki software, such as the restrictions or the timestamp of the last edit, but they are not relevant for our analysis.

The \emph{categorylinks} dump contain the membership of pages to categories.
Every row represent a link between a page and a category.
Indeed we can find the id of the page, the name of the category and the type of link (for example, the link ``page'' denotes that a page belongs to a category, while ``subcat'' is used for categories belonging to other categories).
Other fields denotes the timestamp of the link creation and indexing metadata.

\section{Wikimedia pagecounts}
The Wikimedia page view statistics dataset, aka \emph{pagecounts-raw}, can be downloaded directly from the Wikimedia downloads directory\footnote{\url{https://dumps.wikimedia.org/other/pagecounts-raw/}}.
It is a fairly huge source of data: the dataset compressed size referring to the 2014 is 866 GB distributed in 8756 files.
It is generated by the Wikimedia Squid caching servers: every time a reader request a page a log entry is created and the page is served.
Notice that the cache server does not know if the requested page is an article, a special page, a redirect or even if it exists.
Furthermore, if a page request contain spaces, these are replaced with underscores, because the MediaWiki software uses this convention.

It consists of many files, each of them containing the aggregated page view for a specific hour.
Files use a CSV dialect with \emph{spaces} as separators and they are compressed using the gzip format.
The encoding of this file is not documented.
Many of them seem to use UTF-8, while others ISO-8859-2. % chktex 8
We use the first decoder for all the files, and whenever a Unicode endpoint cannot be found, we replace it with the official \textbf{U+FFFD REPLACEMENT CHARACTER}.

Every file is named according the following format:
\begin{verbatim}
    pagecounts-{year}{month}{day}-{hour}{minute}{second}.gz
\end{verbatim}
The content consists of four columns: the Wikimedia project, the requested page, the number of requests for that page in the last hour, and the size of the response.
The Wikimedia project name has of two parts.
The first is the abbreviation of the language of the project (e.g. \textbf{en} for English Wikimedia projects, \textbf{it} for Italian, etc.) and the second is the abbreviation of the name of the project prefixed by a dot (e.g. \textbf{.b} for wikibooks, \textbf{.d} for wiktionary, etc.).
In case of Wikipedia projects, the second part is omitted, and only the language abbreviation is kept.
Notice that this field is not always in lowercase: this is caused by clients that request url with mixed case letters.
A normalization is therefore required.

The second column, the requested page, is usually escaped by client browser replacing special bytes with an \textbf{\%xx} string, where \textbf{xx} is the hex representation, as per RFC 1808~\cite{rfc1808}.
It is necessary to decode this representation, since different client software can decide to encode or not certain characters.
For instances, client can decide to encode the page named \textbf{New\_Year's\_Day} as \textbf{New\_Year's\_Day} of \textbf{New\_Year\%27s\_Eve}.
This must be taken in consideration, since they refer to the same entity.

The third and the fourth column contains respectively the number of times the page has been served in the hour, and the size of bytes transferred to the clients.
This last field is not relevant for our purposes, but it will be kept because it may be useful for some other research.

In Listing~\ref{lst:pagecounts_extract} is shown an sample from the dump of the first hour of 2014.
The first line is straight forward: the page \textbf{Chomsky} has been seen once, and the size of the response was 98\,938 bytes.
The second line exhibit an example of encoding: the actual name of the page was \textbf{Chomsky–Schützenberger\_theorem} and all the non-ascii character plus the dash have been replaced with their hexadecimal representation.

\begin{listing}[]
    \inputminted[breaklines=true]{xml}{assets/pagecounts_extract_first_hour.txt}
    \caption{Extract from the first hour of the 2014 pagecounts-raw dataset (\textbf{pagecounts-20140101-000000.gz})}
    \label{lst:pagecounts_extract}
\end{listing}

\section{Microsoft academic graph}
\label{sec:mag_dataset}
Microsoft release a dump of the dataset powering Microsoft Academic Search in form of CSV files, compressed with zip.
A brief overview of the size of the dataset is shown in Table~\ref{tbl:mag_size}.

\begin{table}[]
\centering
\caption{Size of Microsoft academic graph dataset files.}
\label{tbl:mag_size}
\begin{tabular}{@{}lrrr@{}}
\multicolumn{1}{c}{\textbf{}} & \textbf{Compressed size} & \textbf{Uncompressed size} & \textbf{Compression ratio} \\ \midrule
Papers table                &      9.0 GB &    27.4 GB & 32.71 \% \\
Paper references table      &      7.4 GB &    18.1 GB & 41.02 \% \\
Paper keywords table        &      1.8 GB &     5.3 GB & 34.51 \%
\end{tabular}
\end{table}

There are many entities in this dataset, and in our analysis we make use of most of them.

The \emph{Papers} listing contains information like the name of the paper, the publication date, its \ac{DOI} (if available), the journal or the conference where it appeared on.
There is a total of 120\,887\,833 papers, but only 35\,039\,319 of them contains a \ac{DOI} record (28.99\%) and, most important, only 33\,345\,644 of them have exactly one \ac{DOI} (27.58\%).
The fact that two papers share the same \ac{DOI} should be sought in the method used by Microsoft to index them.
It is important that a good percentage of papers have this identifier because our analysis relies on the fact that we can match publications that appear in Wikipedia, and this is done by matching the \ac{DOI} of a paper.

There is another important listing in this dataset, the \emph{PaperReferences} dump.
This table contains just two columns: the referring and the referred paper ids.

Finally, there are other files that contains information about the \emph{Journals}, \emph{Conferences}, \emph{Authors}, \emph{Keywords}, etc.


\todo{Explain that the ``conferences'' dataset is extremely biased: it seems that there are only compsci conferences}


\section{Software}
\label{sec:software}
\subsection{Sorting pagecounts}
\label{sub:Sorting pagecounts}
A great amount of time has been spent on the pagecounts dataset.
Currently it is partitioned by the timestamp of each hour.
This representation is useful if you need to get the views for a certain hour, but as soon as you want to get the views for an article in the 2014 it is simply not feasible.
Indeed, you would need to sequentially scan all the 8756 files, decompressing them on the fly.
Also, you could not perform any search optimization based on bisection, since the content inside the files is not ordered and because the content is not normalized.
Therefore we decided to reorganize this dataset, normalizing the content and sorting it by project, page and timestamp, without losing information.

The program written for this purposes is a Spark job, using the Python programming language.
Apache Spark is an open source cluster computing framework originally developed in the AMPLab at University of California, Berkeley but was later donated to the Apache Software Foundation.
The basic building block of Spark is the \ac{RDD}, a logical collection of data partitioned across machines.
\ac{RDD} can be created from existing data sources, such as \ac{CSV} files, or by applying transformation like \emph{map}, \emph{filter}, \emph{reduce} on other \acp{RDD}.
Their peculiarity is that transformations can be applied in parallel by multiple \emph{workers}.

The idea is to normalize the content of the input files, then repartition the entities (i.e.\ the tuples containing the statistics for a given page in a project in a specific hour) using the ordering key.
Finally, every output partition is re-ordered locally and the result is saved into a different CSV file.

Our program will indeed create an \ac{RDD} for every input file.
This is implicitly decompressed upon read operations.
Each line of the file is normalized: the project field is converted to lowercase and the page title is decoded according to RFC 1308~\cite{rfc1808}.
This transformation may cause multiple lines that refer to the same entity.
\todo{explain why}
Therefore it is necessary to merge these lines into one, summing the count of the views and the bytes transferred.
A \emph{key function} is also defined: it takes a tuple of the form \mintinline{python}{(timestamp, project, page, counts, bytes)} and returns the \mintinline{python}{(project, page)} key.

To repartition the entities, we need to fix the number of output partition.
This value fixed to the number of input files, let's say $N$.

Now we want to create a \emph{range partitioner}, a function that takes a tuple and returns the index of the new partition it belongs to.
Notice that Python implicitly defines an ordering between tuples by comparing their fields one at a time.
To create this function, we take an ordered sample of the keys of the first \ac{RDD} and enumerate it.
This enumeration is in fact a mapping between a key (project, page) and a partition index.
When looking for the partition for a new tuple $t$, it is sufficient to return the index corresponding to the last key which is less or equal to $t$.
Since the enumeration is ordered, the search can be optimized using the bisect algorithm.

At this point the \emph{union} of the input \acp{RDD} is repartitioned using this function, resulting in a coarse-sorted dataset.
The last operation needed is to sort tuples of each partition locally.
Values of these partition (i.e.\ tuples containing the timestamp, count and bytes) are also sorted by timestamp.
This results in a new \ac{RDD} normalized and sorted by \mintinline{python}{(page, project, timestamp)}.
The last step is to save the result in a CSV file.
The dataset is actually split in multiple gzipped CSV files because we want to avoid to merge partitions.

Since this job is highly parallelizable, we exploit the UniTN Cisca cluster described \todo{describe it}.
Spark includes a cluster management which makes this operation more or less painless.
It is sufficient to start the Spark executable on the workstations and wait for them to connect to the master server.
Once they are ready, it is sufficient to launch the job on the master, and it automatically splits the tasks between the workers.


\todo{Spark, python libs, sqlite}


\section{Infrastructure}
\label{sec:infrastructure}
\todo{My pc, ``adige'', unitn cluster}
